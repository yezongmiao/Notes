# 问题

## 超分辨中，为什么用L1正则，而不用l2正则

因为L2对于图像而言，本身对于误差大的像素点分配更多的惩罚，而对于误差小的像素点的惩罚较小，无关图像内容。这一机制会导致MSE引导下的图像增强结果更加地平滑。

并且L1相对比L2的loss更容易收敛，有助于加快收敛速度。

## Loss的解决方式

1.查看lossfunction 如果损失中有分母或者log，就有可能因为分类的制信息度过高的原因，导致loss过于大，出现Nan或者分母太小出现Nan的问题。合理设计损失函数。

2.查看数据，可能是有脏的数据导致这部分的问题。patchsize比较小的时候会比较经常出现这样的情况。

## 梯度爆炸或者梯度消失

1.少用Sigmoid,网络别太深，这样容易梯度消失，或者用Sigmoid前先用BN层进行归一化。

2.权重的初始化值过大（梯度爆炸），即选择合适的权重初始化方案。

3.梯度裁剪

## 如何避免过拟合

1.数据增强，过拟合可能是因为数据的问题导致网络的拟合曲线泛化能力差。

2.正则化，Loss中加入正则化函数。

3.网络中加入类似与Dropout等层。

4.根据验证集效果提前终止训练。

## 训练的时候数据样本数量不匹配

样本类别不均衡将导致样本量少的分类所包含的特征过少，并很难从中提取规律；即使得到分类模型，也容易产生过度依赖与有限的数据样本而导致过拟合问题，当模型应用到新的数据上时，模型的预测效果也会很差，关注的评估指标变现也会不理想。

1.数据增强，通过对数据进行裁剪，颜色空间，旋转等方式进行数据量的增强。

2.不平衡学习，分治1:然后训练L个分类器；每个分类器使用大类中的一个簇与所有的小类样本进行训练得到；最后对这L个分类器采取少数服从多数对未知类别数据进行分类，如果是连续值（预测），那么采用平均值。

3.不平衡学习，分治2：使用原始数据集训练第一个学习器L1；将L1错分的数据集作为新的数据集训练L2；将L1和L2分类结果不一致的数据作为数据集训练L3；最后测试集上将三个分类器的结果汇总（结合这三个分类器，采用投票的方式来决定分类结果，因此只有当L2与L3都分类为false时，最终结果才为false，否则true。）

4.使用不对于数据分布平衡方面不敏感的算法，例如svm和决策树（为啥不敏感？）

5.代价敏感学习是在原始标准代价损失函数的基础上，增加了一些约束和权重条件，使得最终代价的数值计算朝向一个特定的方向偏置。